{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# install Spacy and a language model\n",
    "\n",
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "# IF YOU ARE USING A NEW APPLE (M1 CPU) COMPUTER use these lines instead.\n",
    "# !pip install -U pip setuptools wheel\n",
    "# !pip install -U 'spacy[apple]'\n",
    "# !python -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SC207 Text Mining\n",
    "## Sentiment Analysis and Entity Recognition\n",
    "### Using Pre-Trained Models for quick text insights\n",
    "\n",
    "The two methods we're using today rely on pre-trained models to quickly pull apart and analyse pieces of text with a high degree of complexity. Trained on millions of examples of text from the internet, archives, books etc. These models go beyond simply looking at what words are being used, and consider the placement of words, their immediate and distant context, their role within sentence structure and more, to make inferences about what the text says, what matters in the text, and what it could imply.\n",
    "\n",
    "#### Tools\n",
    "Today we're using two packages.\n",
    "- [Transformers](https://pypi.org/project/transformers/): Allows us to quickly download a pre-trained model designed specifically for sentiment analysis using the HuggingFace ðŸ¤— [AI model repository](https://huggingface.co/)\n",
    "- [SpaCy](https://spacy.io/): A natural language processing package that relies on its own pre-trained models to provide a large set of text analysis features. Today we'll be using itrs powerful entity recognition system."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "A tricky area to get right. Prior to pre-trained models sentiment was determined by matching specific words to a predefined table that gave each word a score depending on how positive/negative the designers felt the word was. Whilst this worked for simple text, sentiment is often context dependent, can be morphed by sarcasm, and changes over time as lagnuage evolves. Regularly updated models are shown examples of text that have been labelled as either positive or negative by human annotators, and tested to see if they can accurately predict what a human would label a brand new piece of text.\n",
    "\n",
    "We can initialise one of those super complex incredibly difficult to build models, but it will take every bit of our coding skills to do so..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_sentiment = pipeline('sentiment-analysis')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Done."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_sentiment(\"I love every brilliant thing right now. Super happy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_sentiment(\"My name is James\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_sentiment(\"I am very angry\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_sentiment(\"\"\"A New York judge has ordered President Donald Trump to pay $2m (Â£1.6m)\"\"\"\n",
    "              \"\"\" for misusing funds from his charity to finance his 2016 political campaign.\"\"\"\n",
    "              \"\"\" The Donald J Trump Foundation closed down in 2018. Prosecutors had accused it\"\"\"\n",
    "              \"\"\" of working as \"little more than a chequebook\" for Mr Trump's interests.\"\"\"\n",
    "              \"\"\" Charities such as the one Mr Trump and his three eldest children headed cannot\"\"\"\n",
    "              \"\"\" engage in politics, the judge ruled.\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is worth noting three things...\n",
    "1. Text can ONLY be positive or negative under this model, there is no neutral.\n",
    "2. The score does not indicate strength of sentiment. It indicates how confident the model is in its prediction. We'll look at this more later.\n",
    "3. Always consider does it make sense for a text to have a sentiment. The model will always assign one but this doesn't mean it makes sense."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reshaping"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use our twitter dataset and also our community assignments we generated using NetworkX. We can use these later to examine whether sentiment differs between different groups in our retweet network."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets = pd.read_json('trhr.json')\n",
    "tweets.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "communities = pd.read_csv('communities.csv', index_col=0)\n",
    "communities.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Merging together tweet data and community assignments\n",
    "tweets = tweets.merge(communities,how='left', left_on='user_id', right_index=True).dropna(subset=['community'])\n",
    "tweets.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample = tweets.sample(500).copy().reset_index()\n",
    "sample = sample[['created_at','text','community']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample['sentiment'] = get_sentiment(sample['text'].tolist())\n",
    "sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label_score = pd.json_normalize(sample['sentiment'])\n",
    "sample = pd.concat([sample,label_score], axis=1)\n",
    "\n",
    "sample['community'] = sample['community'].astype(int)\n",
    "\n",
    "sample.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visuals \n",
    "### Distribution of Sentiment\n",
    "\n",
    "Basic Frequency of Positive or Negative Label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample.groupby('label').count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.catplot(data=sample, x='label', kind='count')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.crosstab(sample['community'], sample['label'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Frequency split by community"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.crosstab(sample['community'], sample['label'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.catplot(data=sample, y='community',hue='label', kind='count')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "top_5_communities = sample['community'].value_counts().head().index\n",
    "top_5_communities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample = sample[sample['community'].isin(top_5_communities)].copy() # We use .copy to make it a new object rather than a view as we'll probably change this df later"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "sns.catplot(data=sample, y='community',hue='label', kind='count',order=top_5_communities).set(title='Sentiment of Tweets ordered by community tweet freq')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Score Confidence\n",
    "The confidence score indicates how confident the model is in making its prediction. Whilst individual scores are not an indicator of strength of sentiment, we can see if communities are expressing themselves in ways that are obviously positive or negative. Instances where communities may not have high scores do not indicate neutrality, simply that the language is not clearly positive or negative *to the model*."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grouping first by community and running describe on the score column gives us a range of stats per community\n",
    "# Note some communities are only one person large\n",
    "sample.groupby('community')['score'].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.catplot(data=sample, x='community', y='score',kind='box', hue='label', aspect=2,order=top_5_communities)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Average confidence scores per community and label."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "confidence_data = sample.groupby(['community','label']).mean().unstack()\n",
    "confidence_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.heatmap(data=confidence_data, annot=True, linewidths=0.3,  cmap='coolwarm',vmin=0.5, vmax=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentiment over time\n",
    "Can we examine changing sentiment over time? We'll start with the most direct approach to visualising this, and improve where we can."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we start with basic frequencies per hour for each label type."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time_grouper = pd.Grouper(key='created_at',freq='h')\n",
    "time_series_sentiment = sample.groupby([time_grouper,'label']).count().reset_index()\n",
    "time_series_sentiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.relplot(data=time_series_sentiment,x='created_at', y='score', hue='label', kind='line')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "How about improving this so it is one line, that represents the average sentiment, if we imagine that more positively labelled tweets pulls the score up, and negative labelled ones pull the score down."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample['positivity'] = sample['label'].map({'POSITIVE':1, 'NEGATIVE':0})\n",
    "sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "avg_positivity = sample.groupby(time_grouper).mean().reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.relplot(data=avg_positivity,x='created_at', y='positivity', kind='line')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we have low sample rates for some of our hours the score swings wildly from 0 to 1 and back again. We can smooth this into a trend line using the `.rolling` method of pandas."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "avg_positivity['smoothed'] = avg_positivity['positivity'].rolling(3).mean()\n",
    "avg_positivity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.relplot(data=avg_positivity, x='created_at',y='smoothed', kind='line').set(ylim=(0,1))\n",
    "# we can use .set to ensure the y axis runs from 0 to 1 to tell the full story.\n",
    "\n",
    "# We can also slightly rotate the 'tick' labels on the x-axis to make them more readable.\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time_freq = '10min'\n",
    "window_size = 6\n",
    "\n",
    "time_grouper = pd.Grouper(key='created_at', freq=time_freq)\n",
    "avg_positivity = sample.groupby(time_grouper).mean().reset_index()\n",
    "avg_positivity['smoothed'] = avg_positivity['positivity'].rolling(window_size).mean()\n",
    "\n",
    "sns.relplot(data=avg_positivity, x='created_at',y='smoothed', kind='line').set(\n",
    "    title=f'Rolling Average Positivity: sample_rate={time_freq} | window_size={window_size}', ylim=(0,1), xlabel='Tweet Created At', ylabel='Mean Positivity')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "Named entity recognition (NER) is the technique of extracting key entities within a piece of text,\n",
    "- people\n",
    "- places\n",
    "- organisations\n",
    "- dates\n",
    "- values\n",
    "- currencies etc.\n",
    "\n",
    "SpaCy's processing examines each word in context and uses this to predict which tokens likely refer to particular types of entities like people, organisations, dates etc. It is not using any limited list or reference to \"look up\" these entities, but instead identifies them based on contextual cues.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_data = pd.read_csv('sample_news_large.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = nlp(\"\"\"A New York judge has ordered President Donald Trump to pay $2m (Â£1.6m)\"\"\"\\\n",
    "            \"\"\" for misusing funds from his charity to finance his 2016 political campaign.\"\"\"\\\n",
    "            \"\"\" The Donald J Trump Foundation closed down in 2018. Prosecutors had accused it\"\"\"\\\n",
    "            \"\"\" of working as \"little more than a chequebook\" for Mr Trump's interests.\"\"\"\\\n",
    "            \"\"\" Charities such as the one Mr Trump and his three eldest children headed cannot\"\"\"\\\n",
    "            \"\"\" engage in politics, the judge ruled.\"\"\")\n",
    "\n",
    "# Source: https://www.bbc.co.uk/news/world-us-canada-50338231"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we can access the entities with the .ents attribute\n",
    "text.ents"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# every object in the entities list has a text attribute and a label attribute to tell you the type of entity it is.\n",
    "\n",
    "for entity in text.ents:\n",
    "    print(entity.text, entity.label_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# as we're in Jupyter we can also use SpaCy's built in visualiser\n",
    "\n",
    "spacy.displacy.render(text,style='ent', jupyter=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# if you want to save the annotated version of the\n",
    "# text you can save to html using this function.\n",
    "\n",
    "def save_displacy_to_html(doc, filename, style='ent'):\n",
    "    html_data = spacy.displacy.render(doc, style='ent', jupyter=False, page=True)\n",
    "    with open(filename, 'w+', encoding=\"utf-8\") as f:\n",
    "        f.write(html_data)\n",
    "\n",
    "save_displacy_to_html(text, 'test.html', style='ent')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# lets create a function that can extract specific types of entities from a text\n",
    "\n",
    "def entity_extractor(nlp_doc, entity_type=None, allow_duplicates=False):\n",
    "    if entity_type is None:\n",
    "        ents = [(ent.text,ent.label_) for ent in nlp_doc.ents]\n",
    "    else:\n",
    "        ents = [ent.text for ent in nlp_doc.ents if ent.label_ == entity_type.upper()]\n",
    "    if not allow_duplicates:\n",
    "        ents = list(set(ents))\n",
    "    return ents"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_extractor(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_extractor(text, 'person')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "docs = nlp.pipe(text_data['text'])\n",
    "people = [entity_extractor(doc,'person') for doc in docs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_data['people'] = people\n",
    "text_data['people']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "people_data = text_data.explode('people')[['query','people','title']]\n",
    "people_data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# most mentioned people\n",
    "people_data['people'].value_counts()[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# top ten people per group\n",
    "for query,data in people_data.groupby('query'):\n",
    "    print(f\"****{query}****\")\n",
    "    print(data['people'].value_counts()[:10])\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "top_people = people_data.groupby('people',as_index=False).count().nlargest(5,'query')\n",
    "top_people\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.catplot(data=top_people, y='people',x='query', kind='bar',height=5, aspect=2).set(xlabel='Freq', ylabel='Person', title='5 Most Mentioned People')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for query,data in people_data.groupby('query'):\n",
    "    top_for_query = data.groupby('people', as_index=False).count().nlargest(5,'title')\n",
    "    sns.catplot(data=top_for_query,x='title',y='people', kind='bar', aspect=2).set(title=f'{query.title()}: Top 5 People',\n",
    "                                                                         xlabel='freq',\n",
    "                                                                         ylabel='Person')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Thinking of Entities Relationally\n",
    "One of the skills we developed was to use networkx to start thinking relationally about data, and how we can use measures and metrics without necessarily relying purely on visualisation (though visualisation helps!).\n",
    "\n",
    "We can intersect text analysis with network analysis by mapping the co-occurence of different people within news stories, implying that those people whose names co-occur often are probably connected in some way."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "\n",
    "import netwulf as nw"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def filter_by_degree(G, minimum_degree):\n",
    "    scores = G.degree()\n",
    "    to_keep = [node for node,degree in scores if degree >= minimum_degree]\n",
    "    return G.subgraph(to_keep)\n",
    "\n",
    "def weighted_degree_centrality(G, weight_label='weight'):\n",
    "    degree_scores = G.degree(weight=weight_label)\n",
    "    return {name:score for name,score in degree_scores}\n",
    "\n",
    "# Sets the size attribute of our graph to whatever scores are passed in\n",
    "def size_by(G,scores):\n",
    "    nx.set_node_attributes(G,scores, name='size')\n",
    "    return G"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "edge_list = people_data.dropna()[['people']].copy()\n",
    "edge_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "edge_list = people_data[['people']].copy()\n",
    "edge_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "edge_list.rename(columns={'people':'source'}, inplace=True)\n",
    "edge_list['target'] = edge_list.index\n",
    "edge_list.reset_index(drop=True, inplace=True)\n",
    "edge_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(edge_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "G.number_of_nodes()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "G.number_of_nodes()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "G.edges"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bipartite?\n",
    "<img src='https://github.com/Minyall/sc207_2022-23/blob/master/images/bipartite_projection.png?raw=true' align=\"right\" height=\"200\">\n",
    "\n",
    "A bi-partite graph is one where two sets of nodes only ever form edges with nodes outside their set. So in our case, there are only edges between article nodes, and person nodes, never article to article, nor person to person.\n",
    "\n",
    "However we want a graph that is person to person, but that somehow retains the information on how often they co-occur that comes from their common connection to article nodes. The solution is to create a 'projection' of the original graph that connects up those common edges and weights them based on how many original connections there were."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#First we choose which nodes we want to 'keep' after the projection\n",
    "keep_nodes = edge_list['source'].unique().tolist()\n",
    "keep_nodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now we get our projected Graph\n",
    "\n",
    "people_G = bipartite.weighted_projected_graph(G,keep_nodes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "people_G.edges(data=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now we find all edges where there was only 1 co-occurence and drop them\n",
    "\n",
    "drop_edges = [(source,target) for source, target, attributes in people_G.edges(data=True) if attributes['weight'] == 1]\n",
    "drop_edges"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "drop_edges = [(source,target) for source, target, attributes in people_G.edges(data=True) if attributes['weight'] == 1]\n",
    "drop_edges"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "people_G.remove_edges_from(drop_edges)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "people_G.number_of_nodes()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now we remove any noes that have less than 1 degree, i.e. no connections at all.\n",
    "\n",
    "people_G = filter_by_degree(people_G,minimum_degree=1)\n",
    "people_G.number_of_nodes()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Here we create a degree score that accounts for the edge weights of any edges connected to a node\n",
    "# we use our size-by function to quickly change the sizing before visualisation\n",
    "degree_centrality = nx.degree_centrality(people_G)\n",
    "people_G = size_by(people_G, degree_centrality)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# People who co-occur with the highest number unique of people\n",
    "pd.Series(degree_centrality).sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Highest co-occuring pairs\n",
    "nx.to_pandas_edgelist(people_G).sort_values(by='weight', ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The people most likely to be at the centrwe of the stories, they relationally connect together others.\n",
    "betweenness_scores = nx.betweenness_centrality(people_G)\n",
    "pd.Series(betweenness_scores).sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Size by the betweeness centrality and see if we're right with a visual\n",
    "people_G = size_by(people_G,betweenness_scores)\n",
    "\n",
    "stylized_network, config = nw.visualize(people_G)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nw.draw_netwulf(stylized_network, figsize=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "G.edges"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bipartite?\n",
    "<img src='https://github.com/Minyall/sc207_2022-23/blob/master/images/bipartite_projection.png?raw=true' align=\"right\" height=\"200\">\n",
    "\n",
    "A bi-partite graph is one where two sets of nodes only ever form edges with nodes outside their set. So in our case, there are only edges between article nodes, and person nodes, never article to article, nor person to person.\n",
    "\n",
    "However we want a graph that is person to person, but that somehow retains the information on how often they co-occur that comes from their common connection to article nodes. The solution is to create a 'projection' of the original graph that connects up those common edges and weights them based on how many original connections there were."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#First we choose which nodes we want to 'keep' after the projection\n",
    "keep_nodes = edge_list['source'].unique().tolist()\n",
    "keep_nodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now we get our projected Graph\n",
    "\n",
    "people_G = bipartite.weighted_projected_graph(G,keep_nodes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "people_G.edges(data=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now we find all edges where there was only 1 co-occurence and drop them\n",
    "\n",
    "drop_edges = [(source,target) for source, target, attributes in people_G.edges(data=True) if attributes['weight'] == 1]\n",
    "drop_edges"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "drop_edges = [(source,target) for source, target, attributes in people_G.edges(data=True) if attributes['weight'] == 1]\n",
    "drop_edges"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "people_G.remove_edges_from(drop_edges)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "people_G.number_of_nodes()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now we remove any noes that have less than 1 degree, i.e. no connections at all.\n",
    "\n",
    "people_G = filter_by_degree(people_G,minimum_degree=1)\n",
    "people_G.number_of_nodes()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Here we create a degree score that accounts for the edge weights of any edges connected to a node\n",
    "# we use our size-by function to quickly change the sizing before visualisation\n",
    "degree_centrality = nx.degree_centrality(people_G)\n",
    "people_G = size_by(people_G, degree_centrality)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# People who co-occur with the highest number unique of people\n",
    "pd.Series(degree_centrality).sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Highest co-occuring pairs\n",
    "nx.to_pandas_edgelist(people_G).sort_values(by='weight', ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The people most likely to be at the centrwe of the stories, they relationally connect together others.\n",
    "betweenness_scores = nx.betweenness_centrality(people_G)\n",
    "pd.Series(betweenness_scores).sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Size by the betweeness centrality and see if we're right with a visual\n",
    "people_G = size_by(people_G,betweenness_scores)\n",
    "\n",
    "stylized_network, config = nw.visualize(people_G)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nw.draw_netwulf(stylized_network, figsize=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bipartite?\n",
    "<img src='https://github.com/Minyall/sc207_2022-23/blob/master/images/bipartite_projection.png?raw=true' align=\"right\" height=\"200\">\n",
    "\n",
    "A bi-partite graph is one where two sets of nodes only ever form edges with nodes outside their set. So in our case, there are only edges between article nodes, and person nodes, never article to article, nor person to person.\n",
    "\n",
    "However we want a graph that is person to person, but that somehow retains the information on how often they co-occur that comes from their common connection to article nodes. The solution is to create a 'projection' of the original graph that connects up those common edges and weights them based on how many original connections there were."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#First we choose which nodes we want to 'keep' after the projection\n",
    "keep_nodes = edge_list['source'].unique().tolist()\n",
    "keep_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we get our projected Graph\n",
    "\n",
    "people_G = bipartite.weighted_projected_graph(G,keep_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "people_G.edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we find all edges where there was only 1 co-occurence and drop them\n",
    "\n",
    "drop_edges = [(source,target) for source, target, attributes in people_G.edges(data=True) if attributes['weight'] == 1]\n",
    "drop_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "drop_edges = [(source,target) for source, target, attributes in people_G.edges(data=True) if attributes['weight'] == 1]\n",
    "drop_edges"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "people_G.remove_edges_from(drop_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "people_G.number_of_nodes()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now we remove any noes that have less than 1 degree, i.e. no connections at all.\n",
    "\n",
    "people_G = filter_by_degree(people_G,minimum_degree=1)\n",
    "people_G.number_of_nodes()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Here we create a degree score that accounts for the edge weights of any edges connected to a node\n",
    "# we use our size-by function to quickly change the sizing before visualisation\n",
    "degree_centrality = nx.degree_centrality(people_G)\n",
    "people_G = size_by(people_G, degree_centrality)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# People who co-occur with the highest number unique of people\n",
    "pd.Series(degree_centrality).sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Highest co-occuring pairs\n",
    "nx.to_pandas_edgelist(people_G).sort_values(by='weight', ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The people most likely to be at the centrwe of the stories, they relationally connect together others.\n",
    "betweenness_scores = nx.betweenness_centrality(people_G)\n",
    "pd.Series(betweenness_scores).sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Size by the betweeness centrality and see if we're right with a visual\n",
    "people_G = size_by(people_G,betweenness_scores)\n",
    "\n",
    "stylized_network, config = nw.visualize(people_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nw.draw_netwulf(stylized_network, figsize=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}