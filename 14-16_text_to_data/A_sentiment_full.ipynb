{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install transformers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# install Spacy and a language model\n",
    "\n",
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "# IF YOU ARE USING A NEW APPLE (M1 CPU) COMPUTER use these lines instead.\n",
    "# pip install -U pip setuptools wheel\n",
    "# pip install -U 'spacy[apple]'\n",
    "# python -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SC207 Text Mining\n",
    "## Sentiment Analysis and Entity Recognition\n",
    "### Using Pre-Trained Models for quick text insights\n",
    "\n",
    "The two methods we're using today rely on pre-trained models to quickly pull apart and analyse pieces of text with a high degree of complexity. Trained on millions of examples of text from the internet, archives, books etc. These models go beyond simply looking at what words are being used, and consider the placement of words, their immediate and distant context, their role within sentence structure and more, to make inferences about what the text says, what matters in the text, and what it could imply.\n",
    "\n",
    "#### Tools\n",
    "Today we're using two packages.\n",
    "- [Transformers](https://pypi.org/project/transformers/): Allows us to quickly download a pre-trained model designed specifically for sentiment analysis using the HuggingFace ðŸ¤— [AI model repository](https://huggingface.co/)\n",
    "- [SpaCy](https://spacy.io/): A natural language processing package that relies on its own pre-trained models to provide a large set of text analysis features. Today we'll be using itrs powerful entity recognition system."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "A tricky area to get right. Prior to pre-trained models sentiment was determined by matching specific words to a predefined table that gave each word a score depending on how positive/negative the designers felt the word was. Whilst this worked for simple text, sentiment is often context dependent, can be morphed by sarcasm, and changes over time as lagnuage evolves. Regularly updated models are shown examples of text that have been labelled as either positive or negative by human annotators, and tested to see if they can accurately predict what a human would label a brand new piece of text.\n",
    "\n",
    "We can initialise one of those super complex incredibly difficult to build models, but it will take every bit of our coding skills to do so..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_sentiment = pipeline('sentiment-analysis')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Done."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_sentiment(\"I love every brilliant thing right now. Super happy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_sentiment(\"My name is James\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_sentiment(\"I am very angry\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is worth noting two things...\n",
    "1. Text can ONLY be positive or negative under this model, there is no neutral.\n",
    "2. The score does not indicate strength of sentiment. It indicates how confident the model is in its prediction. We'll look at this more later."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Applying it to a whole dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def flatten_nested_dicts(text_data):\n",
    "    dicts = text_data.to_dict(orient='records')\n",
    "    flattened = pd.json_normalize(dicts)\n",
    "    return flattened"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use our twitter dataset and also our community assignments we generated using NetworkX. We can use these later to examine whether sentiment differs between different groups in our retweet network."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets = pd.read_pickle('example_twitter_data.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets = tweets[tweets['retweeted_status'].isna()] # remove retweets\n",
    "tweets = flatten_nested_dicts(tweets)\n",
    "\n",
    "communities = pd.read_csv('communities.csv', index_col=0)\n",
    "communities.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Merging together tweet data and community assignments\n",
    "tweets = tweets.merge(communities,how='left', left_on='user.screen_name', right_index=True).dropna(subset='community')\n",
    "tweets.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample = tweets.sample(500).copy().reset_index()\n",
    "sample = sample[['full_text','community']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample['sentiment'] = get_sentiment(sample['full_text'].tolist())\n",
    "sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label_score = pd.json_normalize(sample['sentiment'])\n",
    "sample = pd.concat([sample,label_score], axis=1)\n",
    "\n",
    "sample['community'] = sample['community'].astype(int)\n",
    "\n",
    "sample.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visuals \n",
    "### Distribution of Sentiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample.groupby('label').count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.catplot(data=sample, x='label', kind='count')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample.groupby(['community','label']).count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.catplot(data=sample, y='community',hue='label', kind='count')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "order = sample['community'].value_counts().index\n",
    "sns.catplot(data=sample, y='community',hue='label', kind='count',order=order).set(title='Sentiment of Tweets ordered by community tweet freq')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Score Confidence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample.groupby('community')['score'].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.catplot(data=sample, x='community', y='score',kind='box', hue='label', aspect=2,order=order)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "confidence_data = sample.groupby(['community','label']).mean().unstack()\n",
    "confidence_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.heatmap(data=confidence_data, annot=True, linewidths=0.3,  cmap='coolwarm')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "Named entity recognition (NER) is the technique of extracting key entities within a piece of text,\n",
    "- people\n",
    "- places\n",
    "- organisations\n",
    "- dates\n",
    "- values\n",
    "- currencies etc.\n",
    "\n",
    "SpaCy's processing examines each word in context and uses this to predict which tokens likely refer to particular types of entities like people, organisations, dates etc. It is not using any limited list or reference to \"look up\" these entities, but instead identifies them based on contextual cues.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_data = pd.read_csv('sample_news_large.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trump = nlp(\"\"\"A New York judge has ordered President Donald Trump to pay $2m (Â£1.6m)\"\"\"\\\n",
    "            \"\"\" for misusing funds from his charity to finance his 2016 political campaign.\"\"\"\\\n",
    "            \"\"\" The Donald J Trump Foundation closed down in 2018. Prosecutors had accused it\"\"\"\\\n",
    "            \"\"\" of working as \"little more than a chequebook\" for Mr Trump's interests.\"\"\"\\\n",
    "            \"\"\" Charities such as the one Mr Trump and his three eldest children headed cannot\"\"\"\\\n",
    "            \"\"\" engage in politics, the judge ruled.\"\"\")\n",
    "\n",
    "# Source: https://www.bbc.co.uk/news/world-us-canada-50338231"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we can access the entities with the .ents attribute\n",
    "trump.ents"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# every object in the entities list has a text attribute and a label attribute to tell you the type of entity it is.\n",
    "\n",
    "for entity in trump.ents:\n",
    "    print(entity.text, entity.label_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# as we're in Jupyter we can also use SpaCy's built in visualiser\n",
    "\n",
    "spacy.displacy.render(trump,style='ent', jupyter=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# if you want to save the annotated version of the\n",
    "# text you can save to html using this function.\n",
    "\n",
    "def save_displacy_to_html(doc, filename, style='ent'):\n",
    "    html_data = spacy.displacy.render(doc, style='ent', jupyter=False, page=True)\n",
    "    with open(filename, 'w+', encoding=\"utf-8\") as f:\n",
    "        f.write(html_data)\n",
    "\n",
    "save_displacy_to_html(trump, 'test.html', style='ent')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# lets create a function that can extract specific types of entities from a text\n",
    "\n",
    "def entity_extractor(nlp_doc, entity_type=None, count_all=True):\n",
    "    if entity_type is None:\n",
    "        ents = [(ent.text,ent.label_) for ent in nlp_doc.ents]\n",
    "    else:\n",
    "        ents = [ent.text for ent in nlp_doc.ents if ent.label_ == entity_type.upper()]\n",
    "    if not count_all:\n",
    "        ents = list(set(ents))\n",
    "    return ents"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_extractor(trump)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_extractor(trump, 'person')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "docs = nlp.pipe(text_data['text'])\n",
    "people = [entity_extractor(doc,'person') for doc in docs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_data['people'] = people\n",
    "text_data['people']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "people_data = text_data.explode('people')[['query','people','title']]\n",
    "people_data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# most mentioned people\n",
    "people_data['people'].value_counts()[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# top ten people per group\n",
    "for query,data in people_data.groupby('query'):\n",
    "    print(f\"****{query}****\")\n",
    "    print(data['people'].value_counts()[:10])\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "top_people = people_data.groupby('people',as_index=False).count().nlargest(5,'query')\n",
    "top_people\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.catplot(data=top_people, y='people',x='query', kind='bar',height=5, aspect=2).set(xlabel='Freq', ylabel='Person', title='5 Most Mentioned People')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for query,data in people_data.groupby('query'):\n",
    "    top_for_query = data.groupby('people', as_index=False).count().nlargest(5,'title')\n",
    "    sns.catplot(data=top_for_query,x='title',y='people', kind='bar', aspect=2).set(title=f'{query.title()}: Top 5 People',\n",
    "                                                                         xlabel='freq',\n",
    "                                                                         ylabel='Person')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}