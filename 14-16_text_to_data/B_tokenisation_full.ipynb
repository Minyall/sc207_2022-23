{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SC207 Text Mining\n",
    "## Preprocessing and Tokenisation\n",
    "### Preparing text to be used as data\n",
    "\n",
    "- We broadly understand how text might be used as data for qualitative analysis.\n",
    "- Words are not treated simply as individual units of data, but we recognise context, structure, pattern.\n",
    "- How then can text be analysed quantitatively, and how can it be interpreted by a computer to aide that analysis?\n",
    "- The tools we've already used for sentiment analysis and entity recognition performed this process for us in the background, but this won't always be the case.\n",
    "- Usually text analysis techniques require text to be prepared for analysis through two stages\n",
    "\n",
    "### 1. Tokenizing\n",
    "\n",
    "- Computers break down text into units of analysis known as *Tokens*. Tokens are often individual words, but they can also be parts of words, common phrases etc.\n",
    "- Tokenizing is the first fundamental step in any text analysis.\n",
    "- How exactly you split up text is not necessarily straight forward.\n",
    "- There are a range of different strategies which you can see at the [Python NLTK Demo Page](https://text-processing.com/demo/tokenize/).\n",
    "\n",
    "### 2. Pre-Processing\n",
    "- Exactly what happens in pre-processing tends to depend on the type of analysis you are doing.\n",
    "- In general it tends to involve...\n",
    "        - Filtering out of common words and punctuation\n",
    "        - Standardising the text to make it less complex for the computer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 1: Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Example of the problem\n",
    "We already know how to split up a string into a series of items in a list. It's pretty simple using `.split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_phrase = \"I don't see my cat. He has a long tail, fluffy ears and big eyes!\"\\\n",
    "\" He also subscribes to Marxist historical materialism. It's just his way.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_tokens = test_phrase.split()\n",
    "print(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# first we check if the string 'ears' is in the list\n",
    "'ears' in test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# so we would imagine that 'eyes' is also in the list?\n",
    "'eyes' in test_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Having punctuation atached to words like this can cause us problems because the tokens `eyes` and `eyes!` would be considered two seperate things. This messes with a lot of analysis further down the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Using Tokenizers\n",
    "- Tokenizers are functions that split up text for us. \n",
    "- Some of them are based on complex sets of rules, others are based on training computers using lots of examples of text.\n",
    "- There are many (many!) packages available for handling text data. See Moodle for a list of common ones.\n",
    "\n",
    "#### SpaCy\n",
    "\n",
    "We met SpaCy when we used it for named entity recognition. Today we'll also make use of its trained model to perform our tokenisation.\n",
    "\n",
    "We're going to use `en_core_web_md` which means is trained on [petabytes of data from the contemporary internet](https://commoncrawl.org/big-picture/) and so is very up to date in how it understands contemporary language use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# nlp represents the trained language model provided by Spacy...\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# In SpaCy tokenization happens the moment you wrap a string in your language model object nlp()\n",
    "\n",
    "doc = nlp(test_phrase)\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# if we iterate over the doc we can see the tokens.\n",
    "tokens_a = []\n",
    "\n",
    "for word in doc:\n",
    "    tokens_a.append(word)\n",
    "print(tokens_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "**List comprehensions** allow us to do in 1 line what would normally take 3. They are much more efficient than using a `for` loop. We'll see how they can be used more later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokens_b = [word for word in doc]\n",
    "print(tokens_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A number of things have been done by the tokenizer.\n",
    "- Punctuation has been seperated from words into their own tokens.\n",
    "- Words that are contractions of two words (It's > It is / Don't > Do not) have been split into two.\n",
    "- This becomes more useful in a minute and is all part of the process of reducing the nuance of language to make documents more comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 2: Pre-Processing\n",
    "- Whilst the above looks just like the strings again it is actually a SpaCy **Document**.\n",
    "- Once a string is processed by SpaCy it becomes a SpaCy [Document object](https://spacy.io/api/doc). \n",
    "- The SpaCy document object itself is made up of SpaCy [Token objects](https://spacy.io/api/token).\n",
    "\n",
    "This means that Documents and Tokens have a range of associated methods and attributes based on SpaCy's analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's do that again\n",
    "test_phrase = \"I don't see my cat. He has a long tail, fluffy ears and big eyes!\"\\\n",
    "\" He also subscribes to Marxist historical materialism. It's just his way.\"\n",
    "doc = nlp(test_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "doc.lang_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "token = doc[5]\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "type(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Pre-Processing: Lemmatisation\n",
    "Language is very nuanced in real life, but part of the filtering process involves reducing that nuance to strip back to a piece of text's bare bones.\n",
    "\n",
    "```\n",
    "\"I don't like rabbits in space\"\n",
    "\"I do not like rabbits in space\"\n",
    "```\n",
    "- Semantically the same, computationally different.\n",
    "- Lemmatising using the token method `.lemma_` allows us to roll words back to a common 'root'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rabbit_1 = nlp(\"I don't like rabbits in space\")\n",
    "rabbit_2 = nlp(\"I do not like rabbits in space\")\n",
    "\n",
    "print( [token.lemma_ for token in rabbit_1])\n",
    "print( [token.lemma_ for token in rabbit_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "These phrases below are semantically similar, but only share 1 word. Lemmatising brings them closer together computationally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rabbit_1 = nlp(\"I'm loving these rabbits\")\n",
    "rabbit_2 = nlp(\"I love this rabbit!\")\n",
    "\n",
    "print( [token.lemma_ for token in rabbit_1])\n",
    "print( [token.lemma_ for token in rabbit_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's see what it does to our test phrase\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print( [token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Pre-Processing: Punctuation\n",
    "Unless punctuation matters for your analysis (such as needing to break text down into sentences), we normally will clear out punctuation from text. We can use SpaCy's `.is_alpha` attribute to include only tokens that are alphabetical, and still just return the lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processed_doc = [token.lemma_ for token in doc if token.is_alpha]\n",
    "print(processed_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Pre-Processing: Stop Words\n",
    "Stop words are common words that tend to be structurally useful in sentences, but are too common to provide much meaning alone. They are often stripped out before text analysis.\n",
    "Lets add this to our list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = nlp.Defaults.stop_words\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print([word for word in processed_doc if not word.lower in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we'll add .lower() to our result to ensure that we \n",
    "# get rid of any distinction when it comes to capitalisation as well\n",
    "\n",
    "def process_text(doc):\n",
    "    return [token.lemma_.lower() for token in doc if token.is_alpha and token.text != '\\n']\n",
    "    \n",
    "def filter_stops(tokens, stop_words):\n",
    "    return [tok for tok in tokens if tok.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_phrase = \"I don't see my cat. He has a long tail, fluffy ears and big eyes!\"\\\n",
    "\" He also subscribes to Marxist historical materialism. It's just his way.\"\n",
    "doc = nlp(test_phrase)\n",
    "\n",
    "print(process_text(doc))\n",
    "print()\n",
    "print(filter_stops(process_text(doc), stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Processing a Corpus\n",
    "A \"Corpus\" is a collection of textual documents. Often in computational textual analysis a corpus size will run into the hundreds, thousands or even hundreds of thousands. Whilst we could process each document seperately with a `for` loop, it is much more efficient to use Spacy's `pipe` which can process multiple documents in parallel and is memory efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\"I don't like rabbits in space\",\n",
    "         \"I do not like rabbits in space\",\n",
    "         \"I'm loving these rabbits\",\n",
    "         \"I love this rabbit!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "docs = nlp.pipe(corpus)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Spacy has created a generator object. This means that at the moment, no processing has been done. Each document is only processed when we iterate over the generator object as if it were a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We can force the generator to produce the results by iterating over it in a list comprehension.\n",
    "docs = [doc for doc in docs]\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For peace of mind we can check and see that yes the objects are spacy documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    print(type(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Which means we can run our processor on each Spacy doc as it is spat out and retain the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[process_text(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_documents(corpus, stop_words=None):\n",
    "    docs = nlp.pipe(corpus)\n",
    "    processed = [process_text(doc) for doc in docs]\n",
    "    if stop_words is not None:\n",
    "        processed = [filter_stops(doc, stop_words) for doc in processed]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "print( process_documents(corpus) )\n",
    "print()\n",
    "print( process_documents(corpus, stop_words) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Real Data Test\n",
    "So far we've been working on a toy dataset. Lets see what happens with a real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_news_large.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "%time result = process_documents(df['text'], stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df.loc[0,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Phrasing\n",
    "Looks for common co-occurences of tokens in the text and joins them together into single tokens.\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/Archer-phrasing.jpg?raw=true\" align=\"right\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "phraser = Phrases(result, min_count=5)\n",
    "phrased = [phraser[doc] for doc in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for doc in phrased[:5]:\n",
    "    for token in doc:\n",
    "        if '_' in token:\n",
    "            print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Currently we have a list of lists of strings....\n",
    "\n",
    "\n",
    "````\n",
    "Results List\n",
    "    - Document List\n",
    "        - String Token\n",
    "        - String Token\n",
    "        ...\n",
    "    - Document List\n",
    "        - String Token\n",
    "        ....\n",
    "````\n",
    "\n",
    "A cleaner way to store this would be to convert each tokenised document into a string with spaces seperating each token. This can be easily reversed later and is more efficient to store.\n",
    "\n",
    "````\n",
    "Results List\n",
    "    - String of Tokens\n",
    "    - String of Tokens\n",
    "    - String of Tokens\n",
    "    ...\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Takes a list of tokenized documents and joins the tokens together, outputting a list of stringified tokens.\n",
    "def stringify_tokens(tokenized_documents):\n",
    "    return [' '.join(doc) for doc in tokenized_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result_strings = stringify_tokens(phrased)\n",
    "result_strings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# to create a new column of tokenized documents we simply assign the result like so...\n",
    "\n",
    "df['tokens'] = result_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df[['text','tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('sample_news_large_with_tokens.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
