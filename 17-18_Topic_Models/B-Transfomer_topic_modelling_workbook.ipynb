{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Transformer Topic Modelling\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/funko_prime.jpg?raw=true\" align=\"right\" width=\"200\">\n",
    "\n",
    "Whilst topic modelling has been a foundational technique for many years, recent strides in neural networks alongside the building of huge archives of textual material has meant that rather than build our own textual models like before, we can use models pre-trained on millions of examples. These models are far better at accounting for the semantic meaning of words, have a better sense of what words should be given the most attention and account for word context.\n",
    "\n",
    "For example in the sentence \"One dog greeted the other dog\" the values assigned to the first 'dog' will differ to the values assigned to the second 'dog'. BERT refers to...\n",
    "- **B**idirectional - Considers each word and looks both at what preceeds it and what follows it.\n",
    "- **E**ncoder - Encodes the textual material into numerical values...\n",
    "- **R**epresentations from - that accurately represent the original textual meaning\n",
    "- **T**ransformers - a type of machine learning model that is able to adjust what parts of the data it pays most attention to.\n",
    "\n",
    "Transformers have been used for many applications including language translation, and [text generation](https://huggingface.co/gpt2)\n",
    "\n",
    "If you'd like more technical details on the BERT model you can see this [well illustrated guide](https://www.exxactcorp.com/blog/Deep-Learning/how-do-bert-transformers-work), but it is not necessary to fully understand the details, thanks to..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# BERTopic\n",
    "<img src=\"https://maartengr.github.io/BERTopic/logo.png?raw=true\" align=\"right\" width=\"200\">\n",
    "\n",
    "- [BERTopic Website](https://maartengr.github.io/BERTopic/index.html)\n",
    "\n",
    "- Grootendorst, M. (2022) ‘BERTopic: Neural topic modeling with a class-based TF-IDF procedure’. arXiv. Available at: [https://doi.org/10.48550/ARXIV.2203.05794](https://doi.org/10.48550/ARXIV.2203.05794)\n",
    "\n",
    "BERTopic provides us a Python library that leverages BERT transformers whilst providing an accessible set of methods for helpful visualisations, summaries and tweaking of the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! conda install -c conda-forge hdbscan --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! pip install bertopic seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install \"jupyterlab>=3\" \"ipywidgets>=7.6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # May be required if iProgress widget error\n",
    "# ! pip install ipywidgets widgetsnbextension\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"colab\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "news_df = pd.read_csv('sample_news_large_with_tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "raw_corpus = news_df['text'].tolist()\n",
    "raw_corpus[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_corpus = news_df['tokens'].tolist()\n",
    "tokenized_corpus[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Basic BERTopic\n",
    "\n",
    "BERTopic analysis can be broken down into two parts.\n",
    "\n",
    "1. Embeddings\n",
    "2. Topic Representation\n",
    "\n",
    "#### 1. Embeddings\n",
    "BERTopic uses the pre-trained BERT model to generate embeddings for each document. We created embeddings ourselves when we performed vectorisation on text, generating a matrix of document to word count values.\n",
    "\n",
    "\n",
    "> 1. I am a pet dog.\n",
    "> 2. I am a pet cat\n",
    "> 3. Architecture is a serious, serious discipline\n",
    "\n",
    "\n",
    "| **Document** | I | am | a | pet | dog | cat | architecture | is | serious | discipline |\n",
    "|--------------|---|----|---|-----|-----|-----|--------------|----|---------|------------|\n",
    "| 1            | 1 | 1  | 1 | 1   | 1   | 0   | 0            | 0  | 0       | 0          |\n",
    "| 2            | 1 | 1  | 1 | 1   | 0   | 1   | 0            | 0  | 0       | 0          |\n",
    "| 3            | 0 | 0  | 1 | 0   |     |     | 1            | 1  | 2       | 1          |\n",
    "\n",
    "BERT embeddings represent each document as a row of 384 values, generated by the pre-trained model. This data can be used by our computational tools to find clusters of documents based on their semantic similarity, identify which documents are representative of a particular cluster etc. We generate these embeddings by passing in raw text because the BERT model needs context, including all the little words and grammar we might strip out in pre-processing.\n",
    "\n",
    "#### 2. Topic Representation\n",
    "Seperately, BERTopic uses a variation of TFIDF to then generate keywords to represent the topics it finds using the embeddings. In this case TFIDF works best when we **DO** strip out the noise and grammatical features.\n",
    "\n",
    "By default BERTopic uses basic tokenisation with little filtering of words, though this can be adjusted which we'll see later.\n",
    "\n",
    "However, it is also possible to pass it pre-prepared tokens which it will happily use instead. Given we spent all that time learning pre-processing let's use our custom tokens for this notebook, though we'll look at how to tweak BERTopic's built in pre-processing for better results later.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Basic BERTopic - default settings, no custom pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "topic_model =\n",
    "topics, probabilities ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here are our topics. It has discovered 7 topics plus a noise topic labelled -1. There is almost always a noise topic as not all documents perfectly fit into a cluster. There may be outliers or topics that are difficult to classify as one topic or another.\n",
    "\n",
    "Topic representation, which we can see in the labels it has generated for each topic could be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Our first built in visualisation helps us quickly see the topics and their associated words. Hover over the bars to see words and scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Remember the seperation in the model. The embeddings which determine the topics, and then the topic representation. We can update our topic model's topic representation side without impacting the embedding side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Looking better. The number and distribution of the topics is still broadly the same, but now the topic descriptions are improved. Let's work with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Assessing our Topic Model\n",
    "\n",
    "To some extent we already know generally what topics should be in our data as we know the queries that generated the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "query_topic_crosstab =\n",
    "query_topic_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see that generally the topics conform to our queries. This is a good sign indicating that out embeddings were able to accurately determine similarity. We can even see some crossover on particular queries. Remember when the modelling started BERTopic had no idea what query produced each news document, it determined this seperation based on the content of the articles.\n",
    "\n",
    "We won't always have existing classifications like this, but this helps give us confidence that if we did the same procedure on a set of documents in which we had no sense of the topics, it would be able to surface them for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "topic_model.generate_topic_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Topic and Document Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see the similarity of topics using the built in visualiser. Whether they are or are not similar to the extent that they could be merged as a single topic is down to qualitative assessment. Normally they will overlap if they are all part of a larger overarching topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The plot above shows us the distance between topics, with the size of the circle indicating the relative size of the topic in the corpus. Topics that are closer together are considered similar. We can see a more detailed version by visualizing the document embeddings in two dimensons.\n",
    "The first argument specifies how to label the points, rather than relying on the text itself if we provide the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embeddings ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If we examine the scatter plot above more closely, and consider the article titles we can see why some articles might be closer together even within a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Hierarchical Clustering\n",
    "This visual shows us how the topics were determined, indicating where large clusters of documents were split into multiple groups and at what point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see above that topic 4 (Hong kong/Protest) was considered significantly distinct enough to be seperated from the remaining documents early on. Then topic 3 (Tesla). Then topic 5 (online scams and online abuse) then topic 2 (Brexit) and finally topics 0 (facebook/libra) and topic 1 (trump/alt-right). The colouring indicates that these last three topics are more similar to one another than the others, having been split off from a larger cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Term scoring\n",
    "When looking at a topic's keywords, how far down the list do you go until you stop looking. Top 10, top 20? Term rank allows us to see where the number of terms stops adding value to the differentiation of topics. i.e. the point at which adding more terms doesn't aid in differentiating topics anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The guidance is to look for the 'knee' or 'elbow' where the line flattens out. At that point no more terms will improve the differentiation. At this point we can see that differentiation dramatically declines for most topics after only 3 keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Topics over time\n",
    "If you have datestamps for your individual data points, you can get BERTopic to show you topic trends over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "topics_over_time ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that as you hover over each point, the keywords for the topic change. This helps us see how the topic discourse may have altered over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The raw data used to generate the visual is in our topics_over_time dataframe\n",
    "topics_over_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Topics per class\n",
    "Allows us to 'split' up the model to see how different topics might differ depending on some sort of classification. So for example in our data, if we took the time to label each document with the type of publication (Broadsheet newspaper, tabloid, left wing, right wing, etc.) we could see how the topics found across all the documents, differed depending on the type of publisher.\n",
    "\n",
    "We don't have that information(!) but we can demonstrate using our `query` classification at least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "topics_per_class =\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is still informative in that it shows us which topics are most important for each query group, but also that some topics might actually overlap a little. Again note that the words for each topic differ depending on the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ...and again the raw data is available in to us in the variable we created...\n",
    "topics_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Topic Similarity\n",
    "A different way of examining similar phenomena - where do topics overlap, how similar or different are they. Ideally you don't want all your topics to be highly similar, because then you haven't been able to distinguish different topics. However if some overlap in some way, that might tell you something interesting about how different discourses/issues/cultures might overlap or intersect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Topic Distribution\n",
    "If you recall in LDA topic modelling every document has a score for each topic. Whilst most documents might align strongly with only one topic, this approach recognised that topics existed across documents, and one document could contain multiple topics.\n",
    "\n",
    "BERTopic does not work like LDA but it does provide us a table of probabilities. This shows us how probable it is that a document could be classified as topic x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "news_df.loc[3,'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(news_df.loc[3,'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Representative Documents\n",
    "The model stores a few documents that it considers representative of the topic as a whole. We can use the command below, passing in the topic number, to get the corresponding representative documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exporting Analysis for use in your report\n",
    "### Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Any of the tables that are produced by BERTopic can be exported as they are Pandas Dataframes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# In the rare case that the table may instead be a numpy array...\n",
    "type(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# simply wrap in a dataframe and then export\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Figures\n",
    "All figures produced by BERTopic are actually [Plotly](https://plotly.com/python/) figures. They can be exported too..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "topic_bar_chart = topic_model.visualize_barchart(topics=[1,2,3,4],n_words=20, height=400)\n",
    "topic_bar_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Whilst you can write directly to image file in plotly, it requires additional packages. It is simpler to generate the html file and then click the camera icon in the top right of the tool bar that appears when you hover over the figure. This will download an image of the plot for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "On the rare occasion we use a Seaborn chart instead..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "heatmap = sns.heatmap(query_topic_crosstab, cmap='YlGnBu')\n",
    "\n",
    "\n",
    "\n",
    " # dpi 300 or 400 produces a good sized image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Topic Modelling with Twitter Data - Example\n",
    "\n",
    "Here we show an example of how you can apply BERTopic to Twitter data. The small text sizes, noise and variability of twitter data can mean it is difficult to get a handle on whether there are any latent topics of discussion within your collected data. In this example we'll show how you can use the community detection analysis from your network work, to help guide the topic modelling process, as well as some of the more advanced tweaks that can help improve your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here is our original twitter dataset and the communities we detected using NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_json('trhr.json')\n",
    "communities = pd.read_csv('communities.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We may choose to only examine original tweets, excluding retweets. Whether or not this is advisable varies.\n",
    "tweets =\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we combine the original tweet text with the community classifications we generated. We drop any tweets where the user had no community affiliation as that means they weren't a part of our primary network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweets = tweets.merge(communities,how='left', left_on='user_id', right_index=True).dropna(subset=['community'])\n",
    "tweets = tweets[['user_username','text','community']]\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We don't have any pre-existing tokens this time, so we're going to tweak the settings of our BERTopic model to improve the topic representation.\n",
    "BERTopic is built on a range of pre-existing well established components, including SciKit learn's CountVectoriser. We can generate and configure our own vectoriser, and then pass it to our model to be used instead of the default one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Here we will just keep tweets users in the top 5 communities\n",
    "n_communities = 5\n",
    "top_communities = tweets['community'].value_counts().head(n_communities).index\n",
    "tweets = tweets[tweets['community'].isin(top_communities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# We set stop words, tell it to look for phrases between 1 and 2 words long, that a word\n",
    "# must appear at least 5 times to be included, and no more than in 95% of the documents.\n",
    "cv =\n",
    "\n",
    "# We're also going to specify the diversity of the representative words this time.\n",
    "# This pushes the model to try and avoid repeating words across topic representations. It is similar to tweaking the lambda value in LDAvis\n",
    "# nr_topics='auto' tells the model that after the clusters are found, to automatically try and combine highly similar topics.\n",
    "# This can be useful when you get hundreds of topics, or they are highly similar. Conversely if you want fine-grained detail of how\n",
    "# A particular topic may be being discussed in different ways, you may want to avoid this argument.\n",
    "\n",
    "tweet_model =\n",
    "\n",
    "# Here we tell the model that we want it to consider the community assignments when modelling. This is providing guidance to the model that there should be semantic similarity within communities.\n",
    "\n",
    "topics, probabilities =\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweet_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweet_model.visualize_barchart(n_words=20,height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweet_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweet_model.visualize_documents(tweets['text'].tolist(), sample=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweet_model.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweets['community'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "topic_community_crosstab =\n",
    "topic_community_crosstab.loc[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\":(8,6)})\n",
    "sns.clustermap(topic_community_crosstab.loc[0:], cmap='coolwarm', linewidths=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweet_model.get_representative_docs(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "topics_per_class = tweet_model.topics_per_class(tweets['text'].tolist(),topics, classes=tweets['community'].tolist())\n",
    "tweet_model.visualize_topics_per_class(topics_per_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
