{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SC207 Social Data Science\n",
    "# APIs - Gathering Twitter Data\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/tweepy.jpg?raw=true\" align=\"right\" width=\"300\">\n",
    "\n",
    "\n",
    "- API = Application Programming Interface\n",
    "- A Standardised way to retrieve data from platforms.\n",
    "- Many platforms have an API and they all work relatively similarly\n",
    "- Today we will use the package `tweepy` to retrieve data from the Twitter API\n",
    "\n",
    "[Tweepy Documentation](http://docs.tweepy.org/en/stable/)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Installing Tweepy\n",
    "Tweepy is a library that helps us interact with Twitter using Python. Unfortunately it is not installed by default, so we need to install it ourselves. Most of the time you can install new python libraries using the '**Package Installer** in **Python**' or PIP, which stores all the libraries online at the [Python Package Index](https://pypi.org/).\n",
    "\n",
    "Jupyter Lab makes installing from PIP fairly simple.\n",
    "\n",
    "You only need to run this command once. After it has been run tweepy will be installed on your system and won't need reinstalling every time."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install tweepy --upgrade"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepping your credentials storage\n",
    "Generally you want to avoid storing sensitive information, such as API keys, within your code that you may share with others. Whilst there are many solutions to this, a simple one is to store the credentials in a different file which your code can use later.\n",
    "1. Open up the file navigation pane to the left if it's not already open.\n",
    "2. Ensure you are in the folder containing this notebook.\n",
    "3. Right click in some empty space and select 'New File'.\n",
    "4. Rename the file to 'credentials.py' removing the .txt extension completely. You now have a Python file.\n",
    "5. Open the file and in the editor and create one new variable as below, and then save the file.\n",
    "\n",
    "```\n",
    "\n",
    "BEARER_TOKEN = 'PASTE YOUR BEARER TOKEN HERE'\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Here is how we use the credentials from our seperate file, in this notebook.\n",
    "\n",
    "from credentials import BEARER_TOKEN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Connect the API\n",
    "We create a new client object and feed it our bearer token."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Gathering Data - Search\n",
    "Search is one of the simpler ways you can interact with the API.\n",
    "- Search returns a list of tweet objects matching your query\n",
    "- Every request returns up to 100 tweets\n",
    "- You can make 450 requests in a 15 minute window.\n",
    "- A maximum of 45,000 tweets every 15 minutes.\n",
    "- Each request counts against your quota, no matter how many Tweets it returns.\n",
    "- Top limit of 500,000 Tweets per month (so be conservative until you know what you want!)\n",
    "\n",
    "### What you recieve\n",
    "It is important to be clear what Twitter is providing you when you ask for data.\n",
    ">The Twitter's standard search API (search/tweets) allows simple queries against the indices of recent or popular Tweets and behaves similarly to, but not exactly like the Search UI feature available in Twitter mobile or web clients. The Twitter Search API searches against a sampling of recent Tweets published in the past 7 days. Before digging in, itâ€™s important to know that the standard search API is focused on relevance and not completeness. This means that some Tweets and users may be missing from search results.\n",
    "[Twitter API Documentation: Standard Search](https://developer.twitter.com/en/docs/tweets/search/overview/standard)\n",
    "\n",
    "- Already sampled based on 'relevance'.\n",
    "- Max. 7 days old.\n",
    "- NOT complete."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Making a Single Request\n",
    "Lets make a single request for something that will have a lot of results.\n",
    "- `query=` : a string to search for. You can also use [operators](https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators) to make complex queries.\n",
    "\n",
    "Some common operators you can use include...\n",
    "- `AND`\n",
    "- `OR`\n",
    "- Using \"\" to search for a phrase e.g. `\"Elon Musk\"`\n",
    "- Using a (-) dash as a NOT argument. E.g. To get tweets mentioning Elon Musk but not Tesla we would use `\"Elon Musk\" -Tesla`\n",
    "- is: to specify type of tweet for example `is:retweet` will only return retweets. `-is:retweet` will only return non-retweets.\n",
    "\n",
    "[You can view all the argument options in the Tweepy Documentation](http://docs.tweepy.org/en/latest/api.html#search-methods)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "single_response = client.search_recent_tweets(query='society', max_results=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "single_response"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets = single_response.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(tweets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# lets examine just one tweet object\n",
    "\n",
    "single_tweet = tweets[0]\n",
    "\n",
    "single_tweet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Types of Data available for a single Tweet\n",
    "Tweets from the API can contain a range of different relevant data including...\n",
    "- Date/Time posted\n",
    "- The text of the tweet (default)\n",
    "- Edit history of the tweet\n",
    "- Public Metrics such as counted likes, retweets.\n",
    "- Keywords Twitter have identified that might describe the topic, types of people, specific public figures etc known as \"Context Annotations\".\n",
    "- Conversation ID - If it is part of a larger thread, the ID associated with the overall conversation\n",
    "- Entities such as hashtags, urls, mentioned users\n",
    "- An indicator of the Tweet's language\n",
    "- The Source of the Tweet, i.e. the website, the iOS app, the Android app etc.\n",
    "... and more. See the full documentation on [Tweet Fields](https://docs.tweepy.org/en/stable/expansions_and_fields.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's select a few fields to examine their content\n",
    "\n",
    "query = 'society -is:retweet'\n",
    "fields = ['created_at','author_id','referenced_tweets','conversation_id','public_metrics','lang','source','context_annotations','entities']\n",
    "response = client.search_recent_tweets(query,tweet_fields=fields, max_results=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets = response.data\n",
    "single_tweet = tweets[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If we check the type of our single_tweet we can see it is a tweepy Tweet object.\n",
    "# When Tweepy recieved the response from Twitter, it wrapped it up into a useful object for us.\n",
    "type(single_tweet)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "single_tweet.text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "single_tweet.public_metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "single_tweet.lang"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "single_tweet.created_at"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "single_tweet.entities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "single_tweet.context_annotations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If we wrap out list of tweet objects in a Pandas Dataframe, Pandas interprets each object as a row, and the different attributes as columns\n",
    "\n",
    "# More on this later...\n",
    "\n",
    "pd.DataFrame(tweets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tweet Expansions\n",
    "Whilst fields provide additional attributes to our Tweets, expansions 'expand' other objects referenced by the tweet objects. For example...\n",
    "- Data about the tweet author\n",
    "- Data about any referenced Tweets such as retweets, replies etc.\n",
    "- Data about users that are mentioned in tweets\n",
    "- Data about users that are mentioned in referenced tweets\n",
    "- Data about attached media items\n",
    "\n",
    "For a full coverage of all possible expansions see the [Expansions documentation](https://developer.twitter.com/en/docs/twitter-api/expansions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below we're going to call the api again and ask for the author_id expansion to get information about the tweet authors. As well as our regular fields we can request additional fields about the user.\n",
    "The [full list of User fields is available via the documentation](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/user)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query = 'society is:retweet'\n",
    "fields = ['created_at','author_id','referenced_tweets','conversation_id','public_metrics','lang','source','context_annotations','entities']\n",
    "\n",
    "expansions = ['author_id', # This ensures we get info about users that wrote the main tweets we collect\n",
    "              'referenced_tweets.id.author_id'] # This one provides info about authors\n",
    "                                                # of referenced tweets, and the tweets themselves\n",
    "\n",
    "\n",
    "user_fields = ['created_at','public_metrics']\n",
    "\n",
    "\n",
    "\n",
    "response = client.search_recent_tweets(query,\n",
    "                                       tweet_fields=fields, expansions=expansions,\n",
    "                                       user_fields=user_fields,max_results=10)\n",
    "tweets = response.data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "users = response.includes['users']\n",
    "single_user = users[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "single_user"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dict(single_user)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(users)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response.includes['tweets'] # These are the referenced tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(response.includes['tweets'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using the Paginator is similar to our original single_response method.\n",
    "# we first create our Paginator, providing it the client method we want to use,\n",
    "# and any of the arguments we want to be used by that method.\n",
    "\n",
    "query = 'society is:retweet'\n",
    "\n",
    "fields = ['created_at','author_id',\n",
    "          'referenced_tweets','conversation_id',\n",
    "          'public_metrics','lang','source',\n",
    "          'context_annotations','entities']\n",
    "\n",
    "expansions = ['author_id']\n",
    "user_fields = ['created_at','public_metrics']\n",
    "\n",
    "paginator = tweepy.Paginator(client.search_recent_tweets, query=query, tweet_fields=fields,\n",
    "                             user_fields=user_fields, expansions=expansions, max_results=10, limit=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "paginator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using the Paginator is similar to our original single_response method.\n",
    "# we first create our Paginator, providing it the client method we want to use,\n",
    "# and any of the arguments we want to be used by that method.\n",
    "\n",
    "query = 'society is:retweet'\n",
    "\n",
    "fields = ['created_at','author_id',\n",
    "          'referenced_tweets','conversation_id',\n",
    "          'public_metrics','lang','source',\n",
    "          'context_annotations','entities']\n",
    "\n",
    "expansions = ['author_id','referenced_tweets.id.author_id']\n",
    "user_fields = ['created_at','public_metrics']\n",
    "\n",
    "paginator = tweepy.Paginator(client.search_recent_tweets, query=query, tweet_fields=fields,\n",
    "                             user_fields=user_fields, expansions=expansions, max_results=10, limit=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We can examine each list and see how many results and what they look like\n",
    "# Here we should have 20 tweets, 10 results per page, 2 pages.\n",
    "len(tweet_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The most explicit way - using a for loop\n",
    "\n",
    "\n",
    "# This list will capture our primary tweets returned by the query\n",
    "tweet_data = []\n",
    "\n",
    "# This list will capture user data about the authors of our primary tweets\n",
    "user_data = []\n",
    "\n",
    "for response in paginator:\n",
    "    main_tweets = response.data\n",
    "    referenced_tweets = response.includes['tweets']\n",
    "    users = response.includes['users']\n",
    "\n",
    "    tweet_data.extend(main_tweets)\n",
    "    tweet_data.extend(referenced_tweets)\n",
    "    user_data.extend(users)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We can examine each list and see how many results and what they look like\n",
    "#\n",
    "len(tweet_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "tweet_df = pd.DataFrame(tweet_data)\n",
    "tweet_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# It's a good idea to drop any duplicates, as there is no guarantee that our referenced tweets aren't also tweets we already collected in our main response.\n",
    "# Each tweet has a unique id number, so we'll drop any duplicates on the id column\n",
    "tweet_df = tweet_df.drop_duplicates(subset=['id'])\n",
    "tweet_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "tweet_df = pd.DataFrame(tweet_data)\n",
    "tweet_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A quick example\n",
    "\n",
    "df_a = pd.DataFrame({'name':['Alice','Bert','Chris','Danielle'],'age':[22,25,28,32]})\n",
    "df_a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_df = pd.DataFrame(user_data).drop_duplicates(subset=['id'])\n",
    "user_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we can check that all the author ids in our tweets have corresponding user data like so\n",
    "all(tweet_df['author_id'].isin(user_df['id']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_b = pd.DataFrame({'name':['Bert','Chris','Danielle','Fliss'],'job':['Baker','Chef','Doctor','Firefighter']})\n",
    "df_b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df_a is on the left, df_b is on the right\n",
    "\n",
    "df_a.merge(df_b, how='left', left_on='name', right_on='name')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_df = pd.DataFrame(user_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# our user df and tweet df have similar column names, which will cause confusion\n",
    "print(tweet_df.columns)\n",
    "print(user_df.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# our user df and tweet df have similar column names, which will cause confusion\n",
    "print(tweet_df.columns)\n",
    "print(user_df.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweet_df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweet_df = tweet_df.merge(user_df, how='left', left_on='author_id', right_on='user_id')\n",
    "tweet_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If we examine one of our columns...\n",
    "\n",
    "tweet_df['entities']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values in the entities columns aren't strings, they're dictionaries..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Here is the first row's value in the 'entities' column\n",
    "tweet_df.loc[0, 'entities']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The type of the value is dict - dictionary.\n",
    "type(tweet_df.loc[0, 'entities'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# and parts of it can be accessed like a dictionary\n",
    "tweet_df.loc[0, 'entities']['mentions']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we were to save this DataFrame as a .csv file, it would have to turn those dictionaries into strings, because .csv's don't understand Python objects. When we reloaded the data from a CSV our entities column would be a column of weird messy strings.\n",
    "\n",
    "### How do we solve this?\n",
    "\n",
    "- A json file is particularly good at dealing with highly *nested* structures, for example, like a dataframe with column containing lists, which themselves contain dictionaries that contain dictionaries(!). We can save our dataframe to disk as a json file using the Pandas Dataframe `.to_json()` method."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweet_df.to_json('my_tweets.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.read_json('my_tweets.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extending your Collection\n",
    "\n",
    "If you want to gather data across a longer period, such as sampling across a week, you may want to pull from the Twitter API once a day. How do we do this without duplicating our data, and how do we easily just add the new data to our dataset, rather than creating a new one each time?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "from pathlib import Path\n",
    "\n",
    "from credentials import BEARER_TOKEN\n",
    "\n",
    "\n",
    "my_data_filename = Path('my_twitter_dataset.json')\n",
    "query = '\"Elon Musk\" -is:retweet'\n",
    "fields = ['created_at','author_id','conversation_id','referenced_tweets',\n",
    "          'public_metrics','lang','source','context_annotations','entities']\n",
    "expansions = ['author_id']\n",
    "user_fields = ['created_at','public_metrics']\n",
    "\n",
    "total_items = 10000\n",
    "items_per_call = 100\n",
    "n_pages = total_items / items_per_call\n",
    "\n",
    "# Create API\n",
    "client = tweepy.Client(BEARER_TOKEN)\n",
    "\n",
    "# First load in your data if you have it, otherwise create a new DataFrame\n",
    "\n",
    "if my_data_filename.exists():\n",
    "    df = pd.read_json(my_data_filename)\n",
    "    n_records = len(df)\n",
    "\n",
    "    # if there is data check to find the largest id in your dataset, this will be the most recent, and the smallest id, this will be the oldest\n",
    "    max_id = df['id'].max()\n",
    "\n",
    "else:\n",
    "    df = pd.DataFrame()\n",
    "    # set max_id to None because on the first run we don't need to provide an id to limit results\n",
    "    max_id = None\n",
    "    n_records = 0\n",
    "\n",
    "# Pull results from the Twitter API\n",
    "\n",
    "paginator = tweepy.Paginator(client.search_recent_tweets, query=query, tweet_fields=fields,\n",
    "                             user_fields=user_fields,\n",
    "                             expansions=expansions,\n",
    "                             max_results=items_per_call,\n",
    "                             limit=n_pages,\n",
    "                             # since_id=max_id, # ensures to return only Tweets published after your most recently collected tweets\n",
    "                             )\n",
    "\n",
    "# Get primary data...\n",
    "tweet_results = []\n",
    "user_results = []\n",
    "\n",
    "for response in paginator:\n",
    "    tweets = response.data\n",
    "    if response.data is None:\n",
    "        break\n",
    "    users = response.includes['users']\n",
    "\n",
    "    tweet_results.extend(tweets)\n",
    "    user_results.extend(users)\n",
    "\n",
    "temporary_df = pd.DataFrame(tweet_results).drop_duplicates(subset=['id'])\n",
    "user_df = pd.DataFrame(user_results).add_prefix('user_')\n",
    "temporary_df = temporary_df.merge(user_df, how='left', left_on='author_id', right_on='user_id')\n",
    "\n",
    "# Append the new data onto the end of the loaded data (or the empty dataframe if this is the first run)\n",
    "df = pd.concat([df, temporary_df])\n",
    "\n",
    "# Check the dataset for any duplicates by dropping any rows with duplicate ids\n",
    "df = df.drop_duplicates('id',ignore_index=True)\n",
    "\n",
    "# Save back to disk\n",
    "df.to_json(my_data_filename)\n",
    "\n",
    "print(f'Dataset has {len(df)} entries, an increase of {len(df) - n_records}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "from pathlib import Path\n",
    "\n",
    "from credentials import BEARER_TOKEN\n",
    "\n",
    "\n",
    "my_data_filename = Path('my_tweets.json')\n",
    "query = 'society'\n",
    "fields = ['created_at','author_id','conversation_id','referenced_tweets',\n",
    "          'public_metrics','lang','source','context_annotations','entities']\n",
    "expansions = ['author_id','referenced_tweets.id.author_id']\n",
    "user_fields = ['created_at','public_metrics']\n",
    "\n",
    "total_items = 10000\n",
    "items_per_call = 100\n",
    "n_pages = total_items / items_per_call\n",
    "\n",
    "# Create API\n",
    "client = tweepy.Client(BEARER_TOKEN)\n",
    "\n",
    "# First load in your data if you have it, otherwise create a new DataFrame\n",
    "\n",
    "if my_data_filename.exists():\n",
    "    df = pd.read_json(my_data_filename)\n",
    "    n_records = len(df)\n",
    "\n",
    "    # if there is data check to find the largest id in your dataset, this will be the most recent, and the smallest id, this will be the oldest\n",
    "    max_id = df['id'].max()\n",
    "\n",
    "else:\n",
    "    df = pd.DataFrame()\n",
    "    # set max_id to None because on the first run we don't need to provide an id to limit results\n",
    "    max_id = None\n",
    "    n_records = 0\n",
    "\n",
    "# Pull results from the Twitter API\n",
    "\n",
    "paginator = tweepy.Paginator(client.search_recent_tweets, query=query, tweet_fields=fields,\n",
    "                             user_fields=user_fields,\n",
    "                             expansions=expansions,\n",
    "                             max_results=items_per_call,\n",
    "                             limit=n_pages,\n",
    "                             # since_id=max_id, # ensures to return only Tweets published after your most recently collected tweets\n",
    "                             )\n",
    "\n",
    "# Get primary data...\n",
    "tweet_results = []\n",
    "user_results = []\n",
    "\n",
    "for response in paginator:\n",
    "    tweets = response.data\n",
    "    if response.data is None:\n",
    "        break\n",
    "    users = response.includes['users']\n",
    "    referenced_tweets = response.includes['tweets']\n",
    "    tweet_results.extend(tweets)\n",
    "    tweet_results.extend(referenced_tweets)\n",
    "    user_results.extend(users)\n",
    "\n",
    "temporary_df = pd.DataFrame(tweet_results).drop_duplicates(subset=['id'])\n",
    "user_df = pd.DataFrame(user_results).drop_duplicates(subset=['id']).add_prefix('user_')\n",
    "temporary_df = temporary_df.merge(user_df, how='left', left_on='author_id', right_on='user_id')\n",
    "\n",
    "# Append the new data onto the end of the loaded data (or the empty dataframe if this is the first run)\n",
    "df = pd.concat([df, temporary_df])\n",
    "\n",
    "# Check the dataset for any duplicates by dropping any rows with duplicate ids\n",
    "df = df.drop_duplicates('id',ignore_index=True)\n",
    "\n",
    "# Save back to disk\n",
    "df.to_json(my_data_filename)\n",
    "\n",
    "print(f'Dataset has {len(df)} entries, an increase of {len(df) - n_records}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}