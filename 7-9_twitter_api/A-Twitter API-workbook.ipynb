{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SC207 - Session 6\n",
    "# APIs - Gathering Twitter Data\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/tweepy.jpg?raw=true\" align=\"right\" width=\"300\">\n",
    "\n",
    "\n",
    "- API = Application Programming Interface\n",
    "- A Standardised way to retrieve data from platforms.\n",
    "- Many platforms have an API and they all work relatively similarly\n",
    "- Today we will use the package `tweepy` to retrieve data from the Twitter API\n",
    "\n",
    "[Tweepy Documentation](http://docs.tweepy.org/en/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Tweepy\n",
    "Tweepy is a library that helps us interact with Twitter using Python. Unfortunately it is not installed by default, so we need to install it ourselves. Most of the time you can install new python libraries using the '**Package Installer** in **Python**' or PIP, which stores all the libraries online at the [Python Package Index](https://pypi.org/).\n",
    "\n",
    "Jupyter Lab makes installing from PIP fairly simple.\n",
    "\n",
    "You only need to run this command once. After it has been run tweepy will be installed on your system and won't need reinstalling every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports\n",
    "\n",
    "Today we will be using Tweepy and Pandas to retrieve, store and explore data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "\n",
    "# This function is here just to make the class go smoothly!\n",
    "def find_first_retweet(list_of_tweets):\n",
    "    for tweet in list_of_tweets:\n",
    "        if 'retweeted_status' in tweet._json:\n",
    "            return tweet\n",
    "        \n",
    "def find_first_regular_tweet(list_of_tweets):\n",
    "    for tweet in list_of_tweets:\n",
    "        if 'retweeted_status' not in tweet._json:\n",
    "            return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepping your credentials storage\n",
    "Generally you want to avoid storing sensitive information, such as API keys, within your code that you may share with others. Whilst there are many solutions to this, a simple one is to store the credentials in a different file which your code can use later.\n",
    "1. Open up the file navigation pane to the left if it's not already open.\n",
    "2. Right click in some empty space and select 'New File'.\n",
    "3. Rename the file to 'credentials.py' removing the .txt extension completely. You now have a Python file.\n",
    "4. Open the file and in the editor and create two new variables as below, and then save the file.\n",
    "\n",
    "```\n",
    "API_KEY = ''\n",
    "API_SECRET = ''\n",
    "\n",
    "```\n",
    "\n",
    "We'll come back to this file in a minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Authorising and Connecting the API\n",
    "`Tweepy` makes this process incredibly streamlined into essentially three simple stages.\n",
    "\n",
    "### a) Identify your Access Tokens\n",
    "APIs require authorisation tokens to identify who is using the API and to manage API usage by a single account holder.\n",
    "- Go to https://developer.twitter.com\n",
    "- Sign in with your Twitter account details\n",
    "- You may have to navigate back to the Twitter developer page if you get redirected to normal Twitter.\n",
    "- Once signed in select 'Projects & Apps' and then 'Overview' from the left-side menu.\n",
    "- Under Standalone Apps click '+ Create App'.\n",
    "- Give your app a unique name, we suggest your essex username.\n",
    "- Now copy and paste the API Key, and the API Key Secret into the strings in your credentials.py file you created earlier, and save the file. Make sure you are happy with the file before leaving the keys & tokens page in your browser. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how we use the credentials from our seperate file, in this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Create an Authorisation Object\n",
    "We create a special authorisation handler to store our keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.AppAuthHandler(API_KEY, API_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Connect the API\n",
    "We create a new `API` object and feed it our authorisation handler.\n",
    "\n",
    "We also set two additional arguments...\n",
    "- `wait_on_rate_limit` sets the API to wait if you have maxed out your number of queries, and then resume when the limit is lifted\n",
    "- `wait_on_rate_limit_notify` ensures Tweepy informs you of the wait occuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gathering Data - Search\n",
    "Search is one of the simpler ways you can interact with the API.\n",
    "- Search returns a list of tweet objects matching your query\n",
    "- Every request returns up to 100 tweets\n",
    "- You can make 450 requests in a 15 minute window.\n",
    "- A maximum of 45,000 tweets every 15 minutes.\n",
    "- Each request counts against your quota, no matter how many Tweets it returns.\n",
    "\n",
    "### What you recieve\n",
    "It is important to be clear what Twitter is providing you when you ask for data.\n",
    ">The Twitter's standard search API (search/tweets) allows simple queries against the indices of recent or popular Tweets and behaves similarly to, but not exactly like the Search UI feature available in Twitter mobile or web clients. The Twitter Search API searches against a sampling of recent Tweets published in the past 7 days. Before digging in, itâ€™s important to know that the standard search API is focused on relevance and not completeness. This means that some Tweets and users may be missing from search results.\n",
    "[Twitter API Documentation: Standard Search](https://developer.twitter.com/en/docs/tweets/search/overview/standard)\n",
    "\n",
    "- Already sampled based on 'relevance'.\n",
    "- Max. 7 days old.\n",
    "- NOT complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Single Request\n",
    "Lets make a single request for something that will have a lot of results.\n",
    "\n",
    "- Tweepy has a range of 'arguments' built in to the search function.\n",
    "- `q=` query: a string to search for. You can also use [operators](https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators) to make complex queries.\n",
    "- `result_type=`: set this to either `mixed`, `recent` or `popular`. Note again that Twitter is to some extent pre-sampling for you.\n",
    "    - `popular` - prioritise popular tweets\n",
    "    - `recent`  - prioritise recent tweets\n",
    "    - `mixed` **DEFAULT** - include both popular and recent\n",
    "- `tweet_mode=` Set this to 'extended' to ensure you get the full text of a tweet, otherwise it will be cut off after 140 characters (Tweets can now be 280 characters). If your project doesn't care about tweet text then don't bother including the argument.\n",
    "- `count=` max tweets per request. Defaults to 15, can be set up to 100.\n",
    "- `since_id=` Each tweet has a unique ID. If you provide a tweet's ID number here, it will only return tweets posted AFTER that tweet was posted.\n",
    "- `max_id=` As above, but limits the API to returning tweets posted BEFORE the tweet provided.\n",
    "\n",
    "[You can view all the argument options in the Tweepy Documentation](http://docs.tweepy.org/en/latest/api.html#search-methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_response = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the number of results we got\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets examine just one tweet object\n",
    "\n",
    "single_tweet = \n",
    "\n",
    "single_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You get a LOT of data in one single Tweet of a single response, but it's also a bit unwieldy. \n",
    "# Luckily we can access a nice structured version of this with the ._json attribute attached to each tweet object\n",
    "\n",
    "# we'll use the ._json attribute in later sessions...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Data in a single Tweet object\n",
    "Tweets from the API contain data such as...\n",
    "- Time posted\n",
    "- The text of the tweet\n",
    "- Full details on the User who posted.\n",
    "- Details of any media embedded in the tweet\n",
    "- Details of any hashtags user mentions, urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we check the type of our single_tweet we can see it is a tweepy Status object.\n",
    "# When Tweepy recieved the response from Twitter, it wrapped it up into a useful object for us.\n",
    "\n",
    "type(single_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can access any of these items individually as they are set as attributes of the Status class...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a clean way to see all the relevant attributes is to ask for the json keys...\n",
    "\n",
    "single_tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use Jupyter to help you by using the code completion suggestions\n",
    "# type single_tweet. and then hit Tab on your keyboard to see your options.\n",
    "\n",
    "# single_tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some the values of some items will themselves be other objects, with their own attributes...\n",
    "\n",
    "type(single_tweet.user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tweet.user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tweet.user._json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access these subvalues by just chaining our attribute requests\n",
    "\n",
    "single_tweet.user.screen_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a tweet is a retweet it will also contain another tweet object with all the information on the original tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make sure we are all looking at a retweet by using our handy function - this may be the tweet you were looking at already!\n",
    "\n",
    "single_tweet_with_RT = \n",
    "print(single_tweet_with_RT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.... and therefore we can also access the details of that tweet\n",
    "\n",
    "print(single_tweet_with_RT.created_at)\n",
    "print(single_tweet_with_RT.full_text)\n",
    "print(single_tweet_with_RT.user.screen_name)\n",
    "\n",
    "print('*'*100)\n",
    "\n",
    "print(single_tweet_with_RT.retweeted_status.created_at)\n",
    "print(single_tweet_with_RT.retweeted_status.full_text)\n",
    "print(single_tweet_with_RT.retweeted_status.user.screen_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Multiple Requests\n",
    " - With a single request we can retrieve 100 tweets\n",
    " - What if we want to maximise our data access and make multiple requests\n",
    " - We could make a second request and then join the lists of results together...\n",
    " - However Twitter doesn't know what tweets we already retrieved in the first request, so we might get the same ones again.\n",
    " - Enter Tweepy's `Cursor` object.\n",
    " - The `Cursor` will keep track of where we are in the results stream, handle any api limits and blocks, and keep producing results until it reaches the set limit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "from credentials import API_KEY, API_SECRET\n",
    "\n",
    "auth = \n",
    "api = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was our original way we made a request for data from the API\n",
    "\n",
    "old_approach = api.search_tweets(q='brexit',tweet_mode='extended',result_type='mixed', count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the cursor is similar to our original single_response method.\n",
    "# we first create our custom cursor, providing it the api method we want to use,\n",
    "# and any of the arguments we want to be used by that method.\n",
    "\n",
    "\n",
    "\n",
    "our_cursor = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cursors\n",
    "Cursor objects don't DO anything alone. They are almost like a set of instructions, but the instructions aren't being acted out until we do two things....\n",
    "1. Specify whether we want our results as `items` or `pages`.\n",
    "2. Iterate over the cursor\n",
    "\n",
    "#### 1. Items / Pages\n",
    "\n",
    "Cursors can return either a list of individual result items, or result pages depending on what is best. \n",
    "- Pages returns you a stream of response objects, each containing the maximum number of tweets per request.\n",
    "- Items returns you a stream of tweets, essentially joining together the results of the responses.\n",
    "\n",
    "We set whether we are using pages or items using a method attached to the cursor. The number we pass to the cursor defines the limit, of either pages or items. These arguments would return the same number of tweets, presuming we set our count to 100.\n",
    "\n",
    "`our_cursor.pages(2)`\n",
    "\n",
    "`our_cursor.items(200)`\n",
    "\n",
    "#### 2. Iterating over the Cursor\n",
    "\n",
    "For our purposes asking for the `.items()` is sufficient, now we need to iterate over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most explicit way - using a for loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Managing Tweet Data\n",
    "- It's all well and good having this data and printing out pieces of it, but how do we...\n",
    "- Structure it...\n",
    "- Store it...\n",
    "- and Explore it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets get a fresh set of results with a new cursor. Limit to 500 items.\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we'd like this data now in a Pandas DataFrame so we can work with it. Let's just try and put it in and see what happens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok....partial success.\n",
    "Pandas doesn't understand these `Status` objects we're trying to load into it. \n",
    "Whilst often people will load data into Pandas using .csv files, Pandas can create dataframes from python data structures such as lists and dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flintstones_data = [ {'name':'Fred', 'age':30}, {'name':'Wilma', 'age':27}, {'name':'Barney', 'age':32}, {'name':'Betty', 'age':26}  ]\n",
    "\n",
    "toy_df = \n",
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to somehow convert all of our `status` objects into some sort of Python data structure like our Flintstones data....\n",
    "\n",
    "#### Luckily for us....\n",
    "The `._json` method attached to each `Status` turns the object into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tweet = results[0]\n",
    "print(type(single_tweet._json))\n",
    "single_tweet._json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first create a new list of the transformed Status objects\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try...\n",
    "\n",
    "df = pd.DataFrame(json_results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check, we can see that the columns in the DataFrame, match the names of the attributes in our status objects, meaning each column represents that attribute, and each row represents a single Tweet/Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]._json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Saving Tweet Data\n",
    "\n",
    "We can finally save our data to disk if we like. In this case we're going to save to something called a `pickle` file. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we examine one of our columns...\n",
    "\n",
    "df.entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in the entities columns aren't strings, they're dictionaries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the first row's value in the 'entities' column\n",
    "df.loc[0, 'entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The type of the value is dict - dictionary.\n",
    "type(df.loc[0, 'entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and parts of it can be accessed like a dictionary\n",
    "df.loc[0, 'entities']['user_mentions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/pickle.jpg?raw=true\" align=\"right\" height=\"200\">\n",
    "If we were to save this DataFrame as a .csv file, it would have to turn those dictionaries into strings, because .csv's don't understand Python objects. When we reloaded the data from a CSV our entities column would be a column of weird messy strings.\n",
    "\n",
    "### How do we solve this?\n",
    "\n",
    "# PICKLES!\n",
    "- A pickle file is a saved version of a python object. So long as it is saved and loaded with the same version of Pandas, it will retain all the data exactly in the state it is in now.\n",
    "\n",
    "How do we complete this highly complex procedure?...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle it!\n",
    "df.to_pickle('my_tweet_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending your Collection\n",
    "\n",
    "If you want to gather data across a longer period, such as sampling across a week, you may want to pull from the Twitter API once a day. How do we do this without duplicating our data, and how do we easily just add the new data to our dataset, rather than creating a new one each time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "my_data_filename = Path('twitter_data.pkl')\n",
    "query = 'brexit'\n",
    "n_items = 1000\n",
    "\n",
    "\n",
    "# First load in your data if you have it, otherwise create a new DataFrame\n",
    "\n",
    "if my_data_filename.exists():\n",
    "    df = pd.read_pickle(my_data_filename)\n",
    "    \n",
    "    # if there is data check to find the largest id in your dataset, this will be the most recent\n",
    "    max_id = df['id'].max()\n",
    "else:\n",
    "    df = pd.DataFrame()\n",
    "    # set max_id to None because on the first run we don't need to provide an id to limit results\n",
    "    max_id = None\n",
    "    \n",
    "# Pull results from the Twitter API\n",
    "\n",
    "results = []\n",
    "our_cursor = tweepy.Cursor(api.search_tweets, q=query, count=100, tweet_mode='extended', since_id=max_id)\n",
    "\n",
    "for item in our_cursor.items(n_items):\n",
    "    results.append(item._json)\n",
    "\n",
    "# Load this batch of data into a DataFrame\n",
    "    \n",
    "current_data = pd.DataFrame(results)\n",
    "\n",
    "# Append the new data onto the end of the loaded data (or the empty dataframe if this is the first run)\n",
    "df = df.append(current_data)\n",
    "\n",
    "# Check the dataset for any duplicates by dropping any rows with duplicate ids\n",
    "df = df.drop_duplicates('id')\n",
    "\n",
    "# Save back to disk\n",
    "df.to_pickle(my_data_filename)\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
