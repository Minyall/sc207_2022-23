{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SC207 - Session 7\n",
    "# APIs - Exploring and Summarising Twitter Data\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/tweepy.jpg?raw=true\" align=\"right\" width=\"300\">\n",
    "\n",
    "\n",
    "What kinds of exploratory analysis can we run on social media data? This session covers various examples of the kinds of insights that can be gathered through the analysis of social media data, and how to present those results.\n",
    "\n",
    "[Tweepy Documentation](http://docs.tweepy.org/en/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'trhr.json'\n",
    "\n",
    "tweets = pd.read_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. How many Tweets did I get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Tweet ids are unique so we can count the number of unique Tweets using nunique\n",
    "tweets['id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2 When are they from?\n",
    "Twitter can be a very active place, meaning that whilst it may sound like you have a lot of data it may only represent the last 3 minutes. To better understand our dataset we can use the `created_at` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweets['created_at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Currently these are just strings, so Pandas doesn't know how to interpret them. If we convert them to a special type called `datetime` Pandas will be able to better handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweets['created_at'] = pd.to_datetime(tweets['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweets['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweets['created_at'].describe(datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. How often is it being Tweeted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Whilst our time info is to the second, it is more intuitive to see larger trends by the minute or hour. Grouping by time needs a special object called a `Grouper`.\n",
    "\n",
    "First we create a grouper. We provide it two arguments\n",
    "- The `key` which is the column you want to group by\n",
    "- The `freq` which specifies the time period you want to group by for example 'd' for day, or 'h' for hour, or 'min' for minute.\n",
    "- You can see all the options for freq [here in the Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "time_grouper = pd.Grouper(key='created_at', freq='h')\n",
    "count_per_hour = tweets.groupby(time_grouper,as_index=True)['id'].count().reset_index()\n",
    "count_per_hour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.relplot(x = 'created_at',\n",
    "            y = 'id',\n",
    "            height = 5, aspect = 3,\n",
    "            kind = 'line', lw = 3,\n",
    "            data = count_per_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 What types of activity are we seeing?\n",
    "Is this all uniquely written material, or is this a lot of people engaging with the content of others?\n",
    "\n",
    "Tweets perform different functions.\n",
    "- They might be an utterance into the world completely unconnected from anyone else.\n",
    "- They might be a retweet, amplifying and reposting what someone else has said.\n",
    "- They might be a quote, embedding someone elses Tweet but adding their own commentary.\n",
    "- They might be a reply to another Tweet, acting as a conversation.\n",
    "- Notably, they may be multiple of these things at once..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets['referenced_tweets'].loc[3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Here we build a function that will check what options we have for classification, and then chooses the most fitting based on a hierarchy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def check_type(row):\n",
    "    if row is None:\n",
    "        return 'unconnected'\n",
    "    types = [item['type'] for item in row]\n",
    "    if 'retweeted' in types:\n",
    "        return 'retweet'\n",
    "    elif 'quoted' in types:\n",
    "        return 'quoted'\n",
    "    elif 'replied_to' in types:\n",
    "        return 'reply_to'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets['type'] = tweets['referenced_tweets'].apply(check_type)\n",
    "tweets['type']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweet_type_data = tweets[['id','created_at','type']].copy()\n",
    "tweet_type_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time_grouper = pd.Grouper(key='created_at', freq='h')\n",
    "tweet_type_data_to_plot = tweet_type_data.groupby([time_grouper,'type'])['id'].count().reset_index()\n",
    "tweet_type_data_to_plot.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.relplot(x = 'created_at',\n",
    "            y = 'id',\n",
    "            hue='type',\n",
    "            height = 5, aspect = 3,\n",
    "            kind = 'line', lw = 3,\n",
    "            data = tweet_type_data_to_plot)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. What is the most popular content?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We need to use a couple of functions to unpack our nested columns.\n",
    "- `.to_dict(orient='records')` translates a dataframe into a list of dictionaries, each containing their own nested dictionaries\n",
    "- `pd.json_normalize` can create a Dataframe from a list of dictionaries, and flattens out nested dictionaries into their own columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def flatten_nested_dicts(df):\n",
    "    dicts = df.to_dict(orient='records')\n",
    "    flattened = pd.json_normalize(dicts)\n",
    "    return flattened\n",
    "\n",
    "tweets = flatten_nested_dicts(tweets)\n",
    "tweets.info(verbose=True,show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We won't need all of this, let's just grab a subset of columns\n",
    "keep_columns = ['id','text',\n",
    "                'context_annotations',\n",
    "                'public_metrics.like_count',\n",
    "                'public_metrics.retweet_count',\n",
    "                'public_metrics.quote_count',\n",
    "                'public_metrics.reply_count',\n",
    "                'user_username', 'entities.hashtags','entities.mentions']\n",
    "\n",
    "tweets = tweets[keep_columns]\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tweet_url(screen_name, tweet_id):\n",
    "    return f\"https://twitter.com/{screen_name}/status/{tweet_id}\"\n",
    "\n",
    "def print_top_tweets(data, sort_by):\n",
    "    top_favs = data.sort_values(by=sort_by, ascending=False).head(5)\n",
    "\n",
    "    for index_number, row in top_favs.iterrows():\n",
    "\n",
    "        print('*'*10)\n",
    "        print(\"INDEX:\", index_number)\n",
    "        print(\"USER:\", row['user_username'])\n",
    "        print(\"REPLIES:\", row['public_metrics.reply_count'])\n",
    "        print(\"LIKE:\", row['public_metrics.like_count'])\n",
    "        print(\"RT:\", row['public_metrics.retweet_count'])\n",
    "        print(row['text'])\n",
    "        print(tweet_url(screen_name=row['user_username'], tweet_id=row['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_top_tweets(tweets, 'public_metrics.like_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_top_tweets(tweets, 'public_metrics.retweet_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_top_tweets(tweets, 'public_metrics.reply_count')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6. Most Popular #Hashtags\n",
    "Examining the hashtags of your data can give you a sense of the discourses around a particular topic, and inform you of connectivity to other issues. The first step is to get the hashtags out of their nested data structure.\n",
    "\n",
    "For each entry in `entities.hashtags` we see a list, which if it is not empty, contains a set of dictionaries, and one value in each dictionary, the `text` value, is what we actually want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hashtag_data = tweets[['id','entities.hashtags']]\n",
    "hashtag_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# exploding the dataset puts each item of the nested list into its own row.\n",
    "# This means some Tweets will have multiple rows if they have multiple hashtags.\n",
    "\n",
    "exp_hashtag_data = hashtag_data.explode('entities.hashtags').dropna() # in case any are empty\n",
    "exp_hashtag_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Again we have nested dicts so we can use our function from earlier\n",
    "flat_hashtags = flatten_nested_dicts(exp_hashtag_data)\n",
    "flat_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We can quickly check top hashtags like so...\n",
    "\n",
    "flat_hashtags['entities.hashtags.tag'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# or plot them...\n",
    "top_20 = flat_hashtags['entities.hashtags.tag'].value_counts().head(20).index\n",
    "top_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.catplot(y='entities.hashtags.tag', data=flat_hashtags, order=top_20,\n",
    "            height=5, aspect=1.5, kind='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 7 Most Mentioned Users\n",
    "Similarly we can see what users are most mentioned. Often when big issues hit Twitter, particular key individuals get drawn in as people use their handles to draw their attention to it. Significant amounts of mentioning may also indicate centrality of that user in the wider debate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The steps are the same as above... let's speedrun it!\n",
    "\n",
    "mention_data = tweets[['id','entities.mentions']]\n",
    "mention_data = mention_data\n",
    "mention_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp_mention_data = mention_data.explode('entities.mentions').dropna()\n",
    "exp_mention_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Again we have nested dicts so we can use our function from earlier\n",
    "flat_mentions = flatten_nested_dicts(exp_mention_data)\n",
    "flat_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "flat_mentions['entities.mentions.username'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "top_20 = flat_mentions['entities.mentions.username'].value_counts().iloc[:20].index\n",
    "top_20"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.catplot(y='entities.mentions.username', data=flat_mentions, order=top_20, kind='count', height=5, aspect=1.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Most Popular Topics, Figures, Entities etc.\n",
    "Twitter also provides us 'context annotations'. These are keywords assigned to each Tweet by their AI models that tells us a little about the Tweet. The validity of their classification is not 100% but it may be indicative of certain trends or topics."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "annotations = tweets['context_annotations'].explode()\n",
    "annotations = pd.json_normalize(annotations.dropna())\n",
    "annotations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "annotations['domain.name'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "annotation_types = ['Person','Politician','Brand','TV Shows','Events [Entity Service]','Interests and Hobbies']\n",
    "\n",
    "for annot_type in annotation_types:\n",
    "    subset = annotations[annotations['domain.name'] == annot_type]\n",
    "    top_20 = subset['entity.name'].value_counts().iloc[:20].index\n",
    "    ax = sns.catplot(y='entity.name', data=subset, kind='count', height=5, aspect=1.5, order=top_20)\n",
    "    ax.set(title=f'Top 20 Entities: {annot_type}')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}